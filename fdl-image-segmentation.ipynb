{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "execution": {
     "iopub.execute_input": "2022-01-12T21:50:25.536261Z",
     "iopub.status.busy": "2022-01-12T21:50:25.535661Z",
     "iopub.status.idle": "2022-01-12T21:51:34.320934Z",
     "shell.execute_reply": "2022-01-12T21:51:34.320115Z",
     "shell.execute_reply.started": "2022-01-12T21:50:25.536228Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "import numpy as np\n",
    "import os\n",
    "import PIL\n",
    "import glob \n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "import torch\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms as T\n",
    "import torchvision\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "\n",
    "from PIL import Image\n",
    "import cv2\n",
    "import albumentations as A\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "\n",
    "\n",
    "import time\n",
    "import os\n",
    "from tqdm.notebook import tqdm\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "!pip install segmentation-models-pytorch\n",
    "!pip install torchsummary\n",
    "from torchsummary import summary\n",
    "import segmentation_models_pytorch as smp\n",
    "\n",
    "# Import data\n",
    "train_images = '../input/fdl21-fdl-dsba/train_images/train_images/'\n",
    "train_masks = '../input/fdl21-fdl-dsba/train_masks/train_masks/'\n",
    "test_images = '../input/fdl21-fdl-dsba/test_images/test_images/'\n",
    "\n",
    "n_classes = 25\n",
    "# Create a df to store image_id\n",
    "def create_df(filepath):\n",
    "    name = []\n",
    "    for dirname, _, filenames in os.walk(filepath):\n",
    "        for filename in filenames:\n",
    "            name.append(filename.split('.')[0])\n",
    "    \n",
    "    return pd.DataFrame({'id': name}, index = np.arange(0, len(name)))\n",
    "\n",
    "train_df = create_df(train_images)\n",
    "\n",
    "test_df=create_df(test_images)\n",
    "\n",
    "# Split data\n",
    "X_train, X_val = train_test_split(train_df['id'].values, test_size=0.15, random_state=19)\n",
    "\n",
    "class UAVDataset(Dataset):\n",
    "    \n",
    "    def __init__(self, img_path, mask_path, X, mean, std, transform=None, patch=False):\n",
    "        self.img_path = img_path\n",
    "        self.mask_path = mask_path\n",
    "        self.X = X\n",
    "        self.transform = transform\n",
    "        self.patches = patch\n",
    "        self.mean = mean\n",
    "        self.std = std\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        img = cv2.imread(self.img_path + self.X[idx] + '.jpg')\n",
    "        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "        mask = cv2.imread(self.mask_path + self.X[idx] + '.png', cv2.IMREAD_GRAYSCALE)\n",
    "        if self.transform is not None:\n",
    "            aug = self.transform(image=img, mask=mask)\n",
    "            img = Image.fromarray(aug['image'])\n",
    "            mask = aug['mask']\n",
    "        \n",
    "        if self.transform is None:\n",
    "            img = Image.fromarray(img)\n",
    "        \n",
    "        t = T.Compose([T.ToTensor(), T.Normalize(self.mean, self.std)])\n",
    "        img = t(img)\n",
    "        mask = torch.from_numpy(mask).long()\n",
    "        \n",
    "        if self.patches:\n",
    "            img, mask = self.tiles(img, mask)\n",
    "            \n",
    "        return img, mask\n",
    "    def tiles(self, img, mask):\n",
    "\n",
    "        img_patches = img.unfold(1, 512, 512).unfold(2, 768, 768) \n",
    "        img_patches  = img_patches.contiguous().view(3,-1, 512, 768) \n",
    "        img_patches = img_patches.permute(1,0,2,3)\n",
    "        \n",
    "        mask_patches = mask.unfold(0, 512, 512).unfold(1, 768, 768)\n",
    "        mask_patches = mask_patches.contiguous().view(-1, 512, 768)\n",
    "        \n",
    "        return img_patches, mask_patches\n",
    "\n",
    "\n",
    "img_h, img_w = 40, 30\n",
    "means, stdevs = [], []\n",
    "img_list = []\n",
    " \n",
    "imgs_path = '../input/fdl21-fdl-dsba/train_images/train_images'\n",
    "imgs_path_list = os.listdir(imgs_path)\n",
    " \n",
    "len_ = len(imgs_path_list)\n",
    "i = 0\n",
    "for item in imgs_path_list:\n",
    "    img = cv2.imread(os.path.join(imgs_path,item))\n",
    "    img = cv2.resize(img,(img_w,img_h))\n",
    "    img = img[:, :, :, np.newaxis]\n",
    "    img_list.append(img)\n",
    "    i += 1\n",
    "       \n",
    "imgs = np.concatenate(img_list, axis=3)\n",
    "imgs = imgs.astype(np.float32) / 255.\n",
    " \n",
    "for i in range(3):\n",
    "    pixels = imgs[:, :, i, :].ravel()  # flatten\n",
    "    means.append(np.mean(pixels))\n",
    "    stdevs.append(np.std(pixels))\n",
    "\n",
    "transform_train = A.Compose([# resize \n",
    "                             A.Resize(512, 512, interpolation=cv2.INTER_NEAREST), \n",
    "                             # flip\n",
    "                             A.HorizontalFlip(p=0.8),  # probability to be reviwed\n",
    "                             A.VerticalFlip(p=0.8),\n",
    "                             # rotate\n",
    "                             A.RandomRotate90(p=0.8),\n",
    "                             # transpose\n",
    "                             A.Transpose(p=0.8),\n",
    "                             # contrast \n",
    "                             A.RandomBrightnessContrast(brightness_limit = (-0.2,0.5),\n",
    "                                                         contrast_limit = (-0.2,0.5),\n",
    "                                                         p=0.8),\n",
    "                             # sharpen \n",
    "                             A.Sharpen (p=0.8),\n",
    "                             # crop \n",
    "                             A.RandomCrop(height = 512, width = 512, p=0.00001), \n",
    "                             ]) \n",
    "\n",
    "                              \n",
    "\n",
    "transform_val = A.Compose([A.Resize(512, 512, interpolation=cv2.INTER_NEAREST)\n",
    "                          ])\n",
    "\n",
    "#datasets\n",
    "train_set = UAVDataset(train_images, train_masks, X_train, means, stdevs, transform_train, patch=False)\n",
    "val_set = UAVDataset(train_images, train_masks, X_val, means, stdevs, transform_val, patch=False)\n",
    "\n",
    "#dataloader\n",
    "batch_size= 15\n",
    "\n",
    "train_loader = DataLoader(train_set, batch_size=batch_size, num_workers = 4, shuffle=True)\n",
    "val_loader = DataLoader(val_set, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Descriptive Analysis of the Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "execution": {
     "iopub.execute_input": "2022-01-11T09:47:17.942285Z",
     "iopub.status.busy": "2022-01-11T09:47:17.941856Z",
     "iopub.status.idle": "2022-01-11T09:47:21.547587Z",
     "shell.execute_reply": "2022-01-11T09:47:21.546907Z",
     "shell.execute_reply.started": "2022-01-11T09:47:17.942251Z"
    }
   },
   "outputs": [],
   "source": [
    "#Let's look at one image to see what exactly we need to train on \n",
    "images, masks = iter(train_loader).next()\n",
    "\n",
    "fig,ax = plt.subplots(3,2,dpi=300)\n",
    "\n",
    "ax[0][0].imshow(images[0][0])\n",
    "ax[0][1].imshow(masks[0])\n",
    "ax[0][0].set_title('Image (Red Channel)')\n",
    "ax[0][1].set_title('Mask')\n",
    "\n",
    "ax[1][0].imshow(images[1][0])\n",
    "ax[1][1].imshow(masks[1])\n",
    "ax[1][0].set_title('Image (Red Channel)')\n",
    "ax[1][1].set_title('Mask')\n",
    "\n",
    "ax[2][0].imshow(images[2][0])\n",
    "ax[2][1].imshow(masks[2])\n",
    "ax[2][0].set_title('Image (Red Channel)')\n",
    "ax[2][1].set_title('Mask')\n",
    "\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "execution": {
     "iopub.execute_input": "2022-01-11T17:06:39.161716Z",
     "iopub.status.busy": "2022-01-11T17:06:39.160947Z",
     "iopub.status.idle": "2022-01-11T17:06:39.166899Z",
     "shell.execute_reply": "2022-01-11T17:06:39.16579Z",
     "shell.execute_reply.started": "2022-01-11T17:06:39.161672Z"
    }
   },
   "outputs": [],
   "source": [
    "def to_categorical(y, num_classes):\n",
    "    \"\"\" 1-hot encodes a tensor \"\"\"\n",
    "    return np.eye(num_classes, dtype='uint8')[y]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Network Architecture: U-Net GAN \n",
    "\n",
    "The model which will be using is built on top of a GAN, where the following networks are described as follows: \n",
    "\n",
    "1. Generator: U-Net \n",
    "2. Discriminator: Image GAN "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "execution": {
     "iopub.execute_input": "2022-01-12T20:49:08.020084Z",
     "iopub.status.busy": "2022-01-12T20:49:08.019765Z",
     "iopub.status.idle": "2022-01-12T20:49:08.037137Z",
     "shell.execute_reply": "2022-01-12T20:49:08.036462Z",
     "shell.execute_reply.started": "2022-01-12T20:49:08.020051Z"
    }
   },
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# Define double convolution structure\n",
    "def double_conv(in_c, out_c):\n",
    "    conv = nn.Sequential(\n",
    "        nn.Conv2d(in_c,out_c,3,1,padding='same'),\n",
    "        # Add 2 extra batch normalization layers\n",
    "        nn.BatchNorm2d(out_c),\n",
    "        nn.ReLU(),\n",
    "        nn.Conv2d(out_c,out_c,3,1,padding='same'),\n",
    "        nn.BatchNorm2d(out_c),\n",
    "        nn.ReLU()\n",
    "    )\n",
    "    return conv\n",
    "\n",
    "class Generator(nn.Module):\n",
    "    \n",
    "    def __init__(self):\n",
    "        super(Generator, self).__init__()\n",
    "        \n",
    "        #Downsampling \n",
    "        \n",
    "        self.downlayer1 = double_conv(3, 16)\n",
    "        self.maxpool = nn.MaxPool2d(2)\n",
    "        \n",
    "        #Add dropout layer\n",
    "        self.dropout = nn.Dropout(p=0.1)\n",
    "        \n",
    "        self.downlayer2 = double_conv(16, 32)\n",
    "        \n",
    "        self.downlayer3 = double_conv(32, 64)\n",
    "        \n",
    "        self.downlayer4 = double_conv(64, 128)\n",
    "        \n",
    "        self.downlayer5 = double_conv(128, 256)\n",
    "        \n",
    "        \n",
    "        #UpSampling\n",
    "        self.up_trans1 = nn.ConvTranspose2d(256,128,2,2)\n",
    "        self.up_conv1 = double_conv(256, 128)\n",
    "        \n",
    "        self.up_trans2 = nn.ConvTranspose2d(128,64,2,2)\n",
    "        self.up_conv2 = double_conv(128, 64)\n",
    "        \n",
    "        self.up_trans3 = nn.ConvTranspose2d(64,32,2,2)\n",
    "        self.up_conv3 = double_conv(64, 32)\n",
    "        \n",
    "        self.up_trans4 = nn.ConvTranspose2d(32,16,2,2)\n",
    "        self.up_conv4 = double_conv(32, 16)\n",
    "        \n",
    "        self.output = nn.Conv2d(16,25,kernel_size = 1)\n",
    "        \n",
    "        #normaliza output\n",
    "        self.softmax = nn.Softmax(dim=1)\n",
    "        \n",
    "        \n",
    "    def forward(self,x):\n",
    "        \n",
    "        #encoder\n",
    "        x1 = self.downlayer1(x) #\n",
    "        x2 = self.maxpool(x1)\n",
    "        x3 = self.dropout(x2)\n",
    "        \n",
    "        x4 = self.downlayer2(x3) #\n",
    "        x5 = self.maxpool(x4)\n",
    "        x6 = self.dropout(x5)\n",
    "        \n",
    "        x7 = self.downlayer3(x6) #\n",
    "        x8 = self.maxpool(x7)\n",
    "        x9 = self.dropout(x8)\n",
    " \n",
    "        x10 = self.downlayer4(x9) #\n",
    "        x11 = self.maxpool(x10)\n",
    "        x12 = self.dropout(x11)\n",
    "        \n",
    "        x13 = self.downlayer5(x12)\n",
    "        \n",
    "        \n",
    "        #decoder\n",
    "        x = self.up_trans1(x13)\n",
    "        x = self.up_conv1(torch.cat([x,x10],1))\n",
    "        \n",
    "        x = self.up_trans2(x)\n",
    "        x = self.up_conv2(torch.cat([x,x7],1))\n",
    "        \n",
    "        x = self.up_trans3(x)\n",
    "        x = self.up_conv3(torch.cat([x,x4],1))\n",
    "        \n",
    "        x = self.up_trans4(x)\n",
    "        x = self.up_conv4(torch.cat([x,x1],1))\n",
    "        \n",
    "        output = self.output(x)\n",
    "        output = self.softmax(output)\n",
    "        \n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "execution": {
     "iopub.execute_input": "2022-01-12T19:51:36.3334Z",
     "iopub.status.busy": "2022-01-12T19:51:36.332887Z",
     "iopub.status.idle": "2022-01-12T19:51:36.39017Z",
     "shell.execute_reply": "2022-01-12T19:51:36.389318Z",
     "shell.execute_reply.started": "2022-01-12T19:51:36.333365Z"
    }
   },
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "execution": {
     "iopub.execute_input": "2022-01-12T21:54:07.401194Z",
     "iopub.status.busy": "2022-01-12T21:54:07.400851Z",
     "iopub.status.idle": "2022-01-12T21:54:07.484145Z",
     "shell.execute_reply": "2022-01-12T21:54:07.483433Z",
     "shell.execute_reply.started": "2022-01-12T21:54:07.401149Z"
    }
   },
   "outputs": [],
   "source": [
    "lr = 0.001\n",
    "\n",
    "model = Generator()\n",
    "model.to(device)\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr = lr)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "epochs = 25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "execution": {
     "iopub.execute_input": "2022-01-12T21:54:11.937283Z",
     "iopub.status.busy": "2022-01-12T21:54:11.937018Z",
     "iopub.status.idle": "2022-01-12T21:54:38.841847Z",
     "shell.execute_reply": "2022-01-12T21:54:38.839971Z",
     "shell.execute_reply.started": "2022-01-12T21:54:11.937256Z"
    }
   },
   "outputs": [],
   "source": [
    "train_loss = 0.0\n",
    "\n",
    "for epoch in range(epochs):\n",
    "\n",
    "    for i, (images, labels) in enumerate(train_loader):\n",
    "        \n",
    "        images = images.to(device=device, dtype=torch.float)\n",
    "        labels = labels.to(device=device, dtype=torch.int64)\n",
    "        \n",
    "        # Clear gradients\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Forward propagation\n",
    "        outputs = model(images)\n",
    "        \n",
    "        # Calculate softmax and ross entropy loss\n",
    "        loss = criterion(outputs, labels)\n",
    "        \n",
    "        # Calculating gradients\n",
    "        loss.backward()\n",
    "        \n",
    "        # Update parameters\n",
    "        optimizer.step()\n",
    "        \n",
    "        train_loss += loss.item()\n",
    "        \n",
    "    # compute average training loss\n",
    "    train_loss = train_loss/len(train_loader)\n",
    "    print(f\"Epoch {epoch+1} Training loss: {train_loss}\")\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Predictions on testing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "execution": {
     "iopub.execute_input": "2022-01-12T21:47:48.853763Z",
     "iopub.status.busy": "2022-01-12T21:47:48.853321Z",
     "iopub.status.idle": "2022-01-12T21:47:48.866854Z",
     "shell.execute_reply": "2022-01-12T21:47:48.86591Z",
     "shell.execute_reply.started": "2022-01-12T21:47:48.853722Z"
    }
   },
   "outputs": [],
   "source": [
    "class TestLoader(Dataset):\n",
    "    \n",
    "    def __init__(self, images, tgt_dim):\n",
    "        self.images = images \n",
    "        self.tgt_dim = tgt_dim\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.images)\n",
    "\n",
    "    def __getitem__(self,idx):\n",
    "        idx_image = self.images[idx]\n",
    "        image = Image.open(idx_image).resize((self.tgt_dim,\n",
    "                                             self.tgt_dim),PIL.Image.NEAREST)\n",
    "        \n",
    "        img = np.array(image).astype(float)\n",
    "        \n",
    "        transform = transforms.Compose([\n",
    "\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize([0.5,0.5,0.5],[0.5,0.5,0.5])\n",
    "        ])\n",
    "        return transform(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "execution": {
     "iopub.execute_input": "2022-01-12T21:47:51.543149Z",
     "iopub.status.busy": "2022-01-12T21:47:51.542892Z",
     "iopub.status.idle": "2022-01-12T21:47:51.551992Z",
     "shell.execute_reply": "2022-01-12T21:47:51.551242Z",
     "shell.execute_reply.started": "2022-01-12T21:47:51.543122Z"
    }
   },
   "outputs": [],
   "source": [
    "#Load the test set \n",
    "test = sorted(glob.glob(test_images+'*.jpg'))\n",
    "test_set = TestLoader(test,512)\n",
    "test_dataloader = torch.utils.data.DataLoader(test_set,shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "execution": {
     "iopub.execute_input": "2022-01-12T21:48:03.43966Z",
     "iopub.status.busy": "2022-01-12T21:48:03.439328Z",
     "iopub.status.idle": "2022-01-12T21:48:32.619974Z",
     "shell.execute_reply": "2022-01-12T21:48:32.617234Z",
     "shell.execute_reply.started": "2022-01-12T21:48:03.439627Z"
    }
   },
   "outputs": [],
   "source": [
    "from itertools import islice\n",
    "#Predict the test set maps\n",
    "test_predictions = []\n",
    "for image in test_dataloader:\n",
    "    with torch.no_grad():\n",
    "        image = image.to(device)\n",
    "        prediction = model(image.float())\n",
    "        prediction = torch.argmax(prediction, dim=1)\n",
    "        test_predictions.append(np.uint8(prediction.cpu().numpy()[0]))\n",
    "\n",
    "#Saving images to output path \n",
    "# os.makedirs('./test_preds')\n",
    "test_dir = './test_pre_4/'\n",
    "test_img_names = sorted([s[:-4] for s in os.listdir(test_images)])\n",
    "d = dict(zip(test_img_names,test_predictions))\n",
    "for name, pred in d.items():\n",
    "    im = Image.fromarray(pred)\n",
    "    im.save(test_dir+name+'.png')\n",
    "    print(\"Image saved: \", test_dir + name + '.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "execution": {
     "iopub.execute_input": "2022-01-12T21:47:58.427163Z",
     "iopub.status.busy": "2022-01-12T21:47:58.426899Z",
     "iopub.status.idle": "2022-01-12T21:47:58.431255Z",
     "shell.execute_reply": "2022-01-12T21:47:58.430585Z",
     "shell.execute_reply.started": "2022-01-12T21:47:58.427136Z"
    }
   },
   "outputs": [],
   "source": [
    "os.makedirs('./test_pre_4')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Preparing data for submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "execution": {
     "iopub.execute_input": "2022-01-12T21:48:36.022275Z",
     "iopub.status.busy": "2022-01-12T21:48:36.02185Z",
     "iopub.status.idle": "2022-01-12T21:48:43.341004Z",
     "shell.execute_reply": "2022-01-12T21:48:43.340272Z",
     "shell.execute_reply.started": "2022-01-12T21:48:36.02224Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "\n",
    "def rle_encode(img):\n",
    "    '''\n",
    "    img: numpy array, 1 - mask, 0 - background\n",
    "    Returns run length as string formated\n",
    "    '''\n",
    "    pixels = img.flatten()\n",
    "    pixels = np.concatenate([[0], pixels, [0]])\n",
    "    runs = np.where(pixels[1:] != pixels[:-1])[0] + 1\n",
    "    runs[1::2] -= runs[::2]\n",
    "    return ' '.join(str(x) for x in runs)\n",
    "\n",
    "def rle_decode(mask_rle, shape):\n",
    "    '''\n",
    "    mask_rle: run-length as string formated (start length)\n",
    "    shape: (height,width) of array to return\n",
    "    Returns numpy array, 1 - mask, 0 - background\n",
    "    '''\n",
    "    s = mask_rle.split()\n",
    "    starts, lengths = [np.asarray(x, dtype=int) for x in (s[0:][::2], s[1:][::2])]\n",
    "    starts -= 1\n",
    "    ends = starts + lengths\n",
    "    img = np.zeros(shape[0]*shape[1], dtype=np.uint8)\n",
    "    for lo, hi in zip(starts, ends):\n",
    "        img[lo:hi] = 1\n",
    "    return img.reshape(shape)\n",
    "\n",
    "\n",
    "def create_rles():\n",
    "    \"\"\"Used for Kaggle submission: predicts and encode all test images\"\"\"\n",
    "    dir = './test_pre_4/'\n",
    "    N = len(list(os.listdir(dir)))\n",
    "    with open('submission_file_4.csv', 'w') as f:\n",
    "        f.write('ImageClassId,rle_mask\\n')\n",
    "        for index, i in enumerate(os.listdir(dir)):\n",
    "            # print('{}/{}'.format(index, N))\n",
    "\n",
    "            mask = Image.open(dir + i)\n",
    "            mask = mask.resize((1024, 1024), resample=Image.NEAREST)\n",
    "            mask = np.array(mask)\n",
    "\n",
    "            for x in range(1, 25):\n",
    "                enc = rle_encode(mask == x)\n",
    "                f.write(f\"{i.split('_')[0]}_{x},{enc}\\n\")\n",
    "\n",
    "create_rles()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
